{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPP5mJEk5TBwDYzXEYmH2sG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to PyTerrier\n",
        "\n",
        "This notebook introduces the Python package `pyterrier` that builds on the Java-based retrieval toolkit Terrier. All of the code can be run on Google Colab and does not require any other data uploads. However, it should also be possible to run this notebook on your local machine without too much technical preparations.\n",
        "\n",
        "Pyterrier is mature software and offers an excellent API that is easy to learn and it provides easy access to IR evaluations, even for those who are new to the topic of IR research. Within the scope of this notebook, we will use Pyterrier to build an experimental environment that will handle the data indexing, retrieval, and evaluation for you.\n",
        "\n",
        "Even though Pyterrier provides bindings for modern neural retrieval methods and features a sophisticated way of how to declare experimental pipelines, we will focus on more traditional lexical-based retrieval methods that provide a solid basis for your ideas of how to improve a baseline ranking by network analysis and bibliometric data.\n",
        "\n",
        "**Scope of this notebook**:\n",
        "\n",
        "- PyTerrier installation and configuration,\n",
        "- Downloading and indexing the TREC Covid / CORD19 collection,\n",
        "- Using the BatchRetrieve transformer for searching an index,\n",
        "- Conducting an Experiment,\n",
        "- Interactive search examples,\n",
        "- Writing custom reranking pipelines based on different criteria,\n",
        "- Short introduction to `ir_datasets`,\n",
        "- Helpful resources at the end of this notebook.\n",
        "\n",
        "**HINT**: Some of the examples are taken from the ECIR21 tutorial of Pyterrier, which is a good resource for those who want to have further material. The link is provided in the resources."
      ],
      "metadata": {
        "id": "0TFamYjtMZUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Pyterrier and initialize the package"
      ],
      "metadata": {
        "id": "58KXAWcAguJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installation, PyTerrier needs to be initialized by the `init()` method as PyTerrier must download Terrier's jar file and start the Java virtual machine."
      ],
      "metadata": {
        "id": "K76wZ09yUer0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaClBHz8M2uA"
      },
      "outputs": [],
      "source": [
        "!pip install python-terrier\n",
        "\n",
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and index the dataset"
      ],
      "metadata": {
        "id": "pwbsGaUDghyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our document collection CORD19 is publicly available and can be downloaded from Semantic Scholar. In the code cell below, PyTerrier handles the download, extraction, and indexing. All of these are standard operations, but still cause a lot of hassle, if you would have to implement them from scratch."
      ],
      "metadata": {
        "id": "J7LcTsg4Bi-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
        "pt_index_path = './indices/cord19'\n",
        "\n",
        "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
        "  indexer = pt.index.IterDictIndexer(pt_index_path, blocks=True)\n",
        "  index_ref = indexer.index(dataset.get_corpus_iter(), \n",
        "                            fields=['title', 'doi', 'abstract'], \n",
        "                            meta=('docno',))\n",
        "  \n",
        "else:\n",
        "  index_ref = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
        "  \n",
        "index = pt.IndexFactory.of(index_ref)"
      ],
      "metadata": {
        "id": "7vieCBmLGX8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run a simple batch retriever"
      ],
      "metadata": {
        "id": "hPBegEjKFjTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run a retrieval method (BM25) for a batch of queries. In a typical IR experiment, the systems are evaluated over 50 (or more) queries as it is also the case for experiments based on TREC Covid. \n",
        "\n",
        "After initialization, simply hand over all the `title`-queries to the BM25-Transformer. It will output a DataFrame with rankings up to a fixed rank (num_results) for each of the 50 queries.\n",
        "\n",
        "An explanation of the column names is given below."
      ],
      "metadata": {
        "id": "QA3m-f8n1Yjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25 = pt.BatchRetrieve(index_ref , wmodel='BM25', num_results=10)\n",
        "res = bm25.transform(dataset.get_topics('title'))\n",
        "res"
      ],
      "metadata": {
        "id": "Hhrm7zP8FMwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `qid`: id of the query or topic\n",
        "- `docid`: Terrier' internal integer for each document\n",
        "- `docno`: the external (string) unique identifier for each document\n",
        "- `score`: score of the retrieval method (match between query and document)\n",
        "- `rank`: a handy attribute showing the descending order by score\n",
        "- `query`: the input query"
      ],
      "metadata": {
        "id": "cs1WSgO2FOuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run an experiment"
      ],
      "metadata": {
        "id": "wsTMs9lEgdvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often it is more interesting to compare a pack of different retrieval systems to determine which one performs best. For this purpose, PyTerrier offers the evaluation of multiple systems in combination with some standard evaluation measures. As PyTerrier also holds the ground-truth relevance labels (qrels), an entire retrieval benchmark can be run by the cell below.\n",
        "\n",
        "The results are returned in a handy DataFrame. Depending on the evaluation use-case you can select different evaluation measures, conduct significance tests and apply the corresponding correction methods."
      ],
      "metadata": {
        "id": "zygjPKu83Zjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TF_IDF = pt.BatchRetrieve(index_ref, wmodel=\"TF_IDF\") \n",
        "BM25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\") \n",
        "DFRee = pt.BatchRetrieve(index_ref, wmodel=\"DFRee\") \n",
        "\n",
        "systems = [\n",
        "    TF_IDF,\n",
        "    BM25,\n",
        "    DFRee\n",
        "]\n",
        "\n",
        "topics = dataset.get_topics('title')\n",
        "\n",
        "qrels = dataset.get_qrels()\n",
        "\n",
        "# eval_metrics=['P_20', 'ndcg_cut_20', 'map']\n",
        "eval_metrics=['map']\n",
        "\n",
        "exp_res = pt.Experiment(\n",
        "    systems,\n",
        "    topics,\n",
        "    qrels,\n",
        "    eval_metrics=eval_metrics,\n",
        "    baseline=0,\n",
        "    correction='bonferroni'\n",
        ")\n",
        "\n",
        "exp_res"
      ],
      "metadata": {
        "id": "lUiIS7d7dg_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive search and calibration"
      ],
      "metadata": {
        "id": "JwGGhKtk2_DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very often, retrieval methods need to be calibrated for the document collection to be more effective. In the example below, you can play with different parameterizations of the BM25 retrieval methods and see by the output of a single query how the ranking changes."
      ],
      "metadata": {
        "id": "OXNBm8EH6rIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BM25 according to Robertson et al.\n",
        "\n",
        " $\\sum_{t \\in q} \\log \\left(1+\\frac{N-d f_{t}+0.5}{d f_{t}+0.5}\\right) \\cdot \\frac{t f_{t d}}{k_{1} \\cdot\\left(1-b+b \\cdot\\left(\\frac{L_{\\text {d}}}{L_{a v g}}\\right)\\right)+t f_{t d}}$\n",
        "\n",
        "$k_1$ calibrates the term frequency scaling. It is a positive tuning parameter.\n",
        "$k_1=0$ means the term frequency is not considered, whereas large values lead to raw term frequency scalings.\n",
        "\n",
        "$b$ ranges between 0 and 1 and scales the term weight by the document length. There is no length normalization, if $b=0$."
      ],
      "metadata": {
        "id": "6MSlLMzXOk14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_b = 0.5\n",
        "_k1 = 0.95\n",
        "\n",
        "bm25 = pt.BatchRetrieve(index_ref , wmodel='BM25', controls={\"c\" : _b, \"bm25.k_1\": _k1, \"bm25.k_3\": 0.75}) \n",
        "\n",
        "query = 'vaccine'\n",
        "res = BM25.search(query)\n",
        "res "
      ],
      "metadata": {
        "id": "1w3iy8dmNO29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boost by publication date"
      ],
      "metadata": {
        "id": "4L8NxrvTwOub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we want to rerank a first-stage ranking by a custom criterion. More specifically, we boost the ranking score by a recency criterion (the publication date)."
      ],
      "metadata": {
        "id": "qXmARa6s9QoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though, the index contains some other document fields besides the full text, it does not contain all of the available metadata provided for the CORD19 dataset. \n",
        "\n",
        "As we did the download earlier, we can easily use the metadata CSV file in the ir_datasets/ subdirectory."
      ],
      "metadata": {
        "id": "ffZ5et6t_Sh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "metadata = pd.read_csv('~/.ir_datasets/cord19/2020-07-16/metadata.csv')\n",
        "metadata"
      ],
      "metadata": {
        "id": "7JfYLbRwJ9wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata.columns"
      ],
      "metadata": {
        "id": "JiYms9Jy0yoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata[metadata['cord_uid'] == res.iloc[0]['docno']]"
      ],
      "metadata": {
        "id": "xMT68Abw0ys-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us define the custom score boost based on the publication year and integrate it into the ranking."
      ],
      "metadata": {
        "id": "lGcmaXWj_xae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def date_boost(docno):\n",
        "  publish_time = metadata[metadata['cord_uid'] == docno]['publish_time'].iloc[0]\n",
        "\n",
        "  if len(publish_time) > 4:\n",
        "    date_object = datetime.strptime(publish_time, '%Y-%m-%d').date()\n",
        "\n",
        "    if date_object.year > 2015:\n",
        "      return 1.5\n",
        "\n",
        "  return 0.75\n",
        "\n",
        "bm25 = pt.BatchRetrieve(index_ref , wmodel='BM25', controls={\"c\" : _b, \"bm25.k_1\": _k1, \"bm25.k_3\": 0.75}, num_results=10)\n",
        "\n",
        "boost = lambda doc: doc['score'] * date_boost(doc['docno'])\n",
        "\n",
        "reranker = bm25 >> pt.apply.doc_score(boost)\n",
        "\n",
        "reranker.search('vaccine')"
      ],
      "metadata": {
        "id": "_JmglP3Mjg09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boost by number of authors"
      ],
      "metadata": {
        "id": "oLqXZP7wQRRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we want to rerank a first-stage ranking by a custom criterion. More specifically, we boost the ranking score by the number of authors a publication has."
      ],
      "metadata": {
        "id": "QRRXcm8fQfA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def author_boost(docno):\n",
        "    raw_authors = metadata[metadata['cord_uid'] == docno]['authors'].iloc[0]\n",
        "\n",
        "    if isinstance(raw_authors, str):\n",
        "      authors = raw_authors.split(';')\n",
        "      num_authors = len(authors)\n",
        "      return num_authors\n",
        "\n",
        "    return 1\n",
        "\n",
        "bm25 = pt.BatchRetrieve(index_ref , wmodel='BM25', controls={\"c\" : _b, \"bm25.k_1\": _k1, \"bm25.k_3\": 0.75}, num_results=10)\n",
        "\n",
        "boost = lambda doc: doc['score'] * author_boost(doc['docno'])\n",
        "\n",
        "reranker = bm25 >> pt.apply.doc_score(boost)\n",
        "\n",
        "reranker.search('vaccine')"
      ],
      "metadata": {
        "id": "rLVZQr3JPygq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boost by citation counts"
      ],
      "metadata": {
        "id": "8qyV-MpVwJBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we want to rerank a first-stage ranking by a custom criterion. More specifically, we boost the ranking score by the citation count of a publication. The purpose of this section should give you some idea about what other kinds of reranking criterions are possible."
      ],
      "metadata": {
        "id": "NqKFuX-P9rDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the citation count, we use `scholarly` an unofficial Python-API to Google Scholar. Please do not hammer the server, otherwise your IP may get blocked. See also the random sleep interval between the request.\n",
        "\n",
        "**Highly experimental: sometimes it works, sometimes not.** Later, it may be more practical to scrape bibliometric data in advance and ingest it into a database that can be queried on purpose."
      ],
      "metadata": {
        "id": "4__xWoeN96Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/scholarly-python-package/scholarly\n",
        "# https://scholarly.readthedocs.io/en/latest/index.html\n",
        "!pip3 install scholarly"
      ],
      "metadata": {
        "id": "E6PXHIXtxssr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scholarly import scholarly \n",
        "import time\n",
        "from random import randint\n",
        "\n",
        "def cite_boost(docno):\n",
        "    _title = metadata[metadata['cord_uid'] == docno]['title'].iloc[0]\n",
        "    pub = scholarly.search_pubs(_title)\n",
        "    time.sleep(randint(2,4))\n",
        "    num_citations = next(pub).get('num_citations')\n",
        "\n",
        "    return num_citations\n",
        "\n",
        "bm25 = pt.BatchRetrieve(index_ref , wmodel='BM25', controls={\"c\" : _b, \"bm25.k_1\": _k1, \"bm25.k_3\": 0.75}, num_results=10)\n",
        "\n",
        "boost = lambda doc: doc['score'] * cite_boost(doc['docno'])\n",
        "\n",
        "reranker = bm25 >> pt.apply.doc_score(boost)\n",
        "\n",
        "reranker.search('vaccine')"
      ],
      "metadata": {
        "id": "UyWsToRou2oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TL;DR: Experimentation with a custom reranking criterion"
      ],
      "metadata": {
        "id": "o9gEScRu4cSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this template as a quickstart for your experiments."
      ],
      "metadata": {
        "id": "sGaG2Dm3__zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-terrier\n",
        "\n",
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()\n",
        "\n",
        "import os\n",
        "\n",
        "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
        "pt_index_path = './indices/cord19'\n",
        "\n",
        "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
        "  indexer = pt.index.IterDictIndexer(pt_index_path, blocks=True)\n",
        "  index_ref = indexer.index(dataset.get_corpus_iter(), \n",
        "                            fields=['title', 'doi', 'date', 'abstract'], \n",
        "                            meta=('docno',))\n",
        "  \n",
        "else:\n",
        "  index_ref = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
        "  \n",
        "index = pt.IndexFactory.of(index_ref)"
      ],
      "metadata": {
        "id": "edQW8kzSLURU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_boost(docno):\n",
        "    # INSERT YOUR IDEAS HERE\n",
        "\n",
        "    return 1\n",
        "\n",
        "\n",
        "baseline = pt.BatchRetrieve(index_ref , wmodel='BM25', num_results=10)\n",
        "\n",
        "boost = lambda doc: doc['score'] * custom_boost(doc['docno'])\n",
        "\n",
        "reranker = baseline >> pt.apply.doc_score(boost)\n",
        "\n",
        "systems = [\n",
        "    baseline,\n",
        "    reranker\n",
        "]\n",
        "\n",
        "names = ['Baseline', 'Reranker']\n",
        "\n",
        "topics = dataset.get_topics('title')\n",
        "\n",
        "qrels = dataset.get_qrels()\n",
        "\n",
        "eval_metrics=['P_20', 'ndcg_cut_20', 'map']\n",
        "\n",
        "exp_res = pt.Experiment(\n",
        "    systems,\n",
        "    topics,\n",
        "    qrels,\n",
        "    eval_metrics,\n",
        "    names=names,\n",
        "    baseline=0\n",
        ")\n",
        "\n",
        "exp_res"
      ],
      "metadata": {
        "id": "xysAjktg4V4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ir_datasets"
      ],
      "metadata": {
        "id": "_8286OIA2jIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ir_datasets` is integrated into PyTerrier and is the major component, which facilitates the data handling. As PyTerrier's API is actually comprehensive enough you do not necessarily need to use `ir_datasets` explicitly, but depending on your implementation ideas, it may be helpful. \n",
        "\n",
        "The example below shows how to get a DocStore from a defined dataset in order to read out single documents by their `docno`."
      ],
      "metadata": {
        "id": "jDFnhv-f7vn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ir_datasets\n",
        "\n",
        "_dataset = ir_datasets.load(\"cord19\")\n",
        "docstore = _dataset.docs_store()\n",
        "\n",
        "res = reranker.search('vaccine')\n",
        "docno = res.iloc[0]['docno']\n",
        "\n",
        "docstore.get(docno)"
      ],
      "metadata": {
        "id": "gn3Kn8RUOVp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing run files to disk"
      ],
      "metadata": {
        "id": "h6DmVWKM2uhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we conduct the IR evaluations in the old-fashioned way with files stored on disk and by the use of the software `trec_eval`. We have included this section for illustrative purposes, so that you can have an idea about what kind of hassle is avoided by the use of PyTerrier. However, depending on your approach, it may be helpful for some evaluations to know the more traditional way."
      ],
      "metadata": {
        "id": "dYBbZiUqWdOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's make rankings for a batch of queries and write it into a text file - a so-called run file."
      ],
      "metadata": {
        "id": "JQ11tcxK28WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25 = pt.BatchRetrieve(index_ref , wmodel='BM25', num_results=1000)\n",
        "res = bm25.transform(dataset.get_topics('title'))\n",
        "res = res.drop_duplicates(subset=['docno']) # we did not remove duplicates from the index\n",
        "run_name = 'bm25'\n",
        "file_name = run_name\n",
        "pt.io.write_results(res, file_name, format='trec',run_name=run_name)\n",
        "!cat bm25"
      ],
      "metadata": {
        "id": "9BsBJ_mBVtpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before evaluation, you need to obtain the ground-truth relevance labels (qrels) from NIST servers."
      ],
      "metadata": {
        "id": "EG0MnI2O51vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ir.nist.gov/trec-covid/data/qrels-covid_d5_j0.5-5.txt && head -n 10 qrels-covid_d5_j0.5-5.txt"
      ],
      "metadata": {
        "id": "xUekVU409eks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While not necessary at this point, it is sometimes helpful to have a look at the queries and corresponding topic texts."
      ],
      "metadata": {
        "id": "pFJZuuIC6CfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ir.nist.gov/trec-covid/data/topics-rnd5.xml && head -n 6 topics-rnd5.xml"
      ],
      "metadata": {
        "id": "YjPHUq249kov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we need to download and compile the evaluation software `trec_eval`."
      ],
      "metadata": {
        "id": "ERNX-KG66NFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/usnistgov/trec_eval.git && cd trec_eval && make"
      ],
      "metadata": {
        "id": "FAFhYAz99t-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the compilation has finished, we can evaluate the run file. "
      ],
      "metadata": {
        "id": "9jGvNqZr6UJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./trec_eval/trec_eval qrels-covid_d5_j0.5-5.txt bm25"
      ],
      "metadata": {
        "id": "mQWJ5vA-94W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5 Transformer-based Reranking"
      ],
      "metadata": {
        "id": "0zUoIpc80Et_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTerrier's extensions offer an out-of-the-box experience for rerankings based on modern neural language models. The example below illustrates how default (not fine-tuned) T5-based rerankers can be integrated into the evaluations."
      ],
      "metadata": {
        "id": "fNJjkUOo5Cop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reranking based on a Mono- or Duo-T5 reranker\n",
        "# https://github.com/terrierteam/pyterrier_t5\n",
        "# https://colab.research.google.com/github/terrierteam/pyterrier_t5/blob/master/pyterrier_t5_trec-covid.ipynb#scrollTo=5fMRKXSjjd1w\n",
        "# https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-t4/t4-tensor-core-datasheet-951643.pdf\n",
        "# see also the documentation: https://pyterrier.readthedocs.io/en/latest/neural.html\n",
        "\n",
        "!pip install --upgrade git+https://github.com/terrierteam/pyterrier_t5.git\n",
        "\n",
        "from pyterrier_t5 import MonoT5ReRanker, DuoT5ReRanker\n",
        "monoT5 = MonoT5ReRanker(text_field='abstract')\n",
        "duoT5 = DuoT5ReRanker(text_field='abstract')\n",
        "\n",
        "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\") % 100\n",
        "mono_pipeline = bm25 >> pt.text.get_text(dataset, \"abstract\") >> monoT5\n",
        "duo_pipeline = mono_pipeline % 10 >> duoT5"
      ],
      "metadata": {
        "id": "xYI-6pkEhUXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pzVDHYxYFz8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TREC Covid and CORD19\n",
        "- **TREC Covid website**: https://ir.nist.gov/trec-covid/\n",
        "- **Data resources**: https://ir.nist.gov/trec-covid/data.html\n",
        "- **CORD19 dataset**: https://github.com/allenai/cord19\n",
        "- **CORD19 paper**: https://api.semanticscholar.org/CorpusID:216056360\n",
        "\n",
        "## Pyterrier\n",
        "- **Pyterrier documentation**: https://pyterrier.readthedocs.io/en/latest/\n",
        "- **Transformer documentation**: https://pyterrier.readthedocs.io/en/latest/transformer.html (**not** the neural language models)\n",
        "- **GitHub repository**: https://github.com/terrier-org/pyterrier\n",
        "- **arXiv publication**: https://arxiv.org/abs/2007.14271\n",
        "- **Javadocs of matching models**: http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html \n",
        "- **ECIR21 tutorial:** https://github.com/terrier-org/ecir2021tutorial\n",
        "\n",
        "## ir_datasets\n",
        "- **Website / catalog**: https://ir-datasets.com/\n",
        "- **CORD19 subpage**: https://ir-datasets.com/cord19.html\n",
        "- **Python API**: https://ir-datasets.com/python.html"
      ],
      "metadata": {
        "id": "BSEBZjquNn8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Terrier's matching models\n",
        "\n",
        "cf. http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html"
      ],
      "metadata": {
        "id": "ALE8vgzVNrfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF_IDF = pt.BatchRetrieve(index, wmodel=\"TF_IDF\") \n",
        "# BM25 = pt.BatchRetrieve(index, wmodel=\"BM25\") \n",
        "# Tf = pt.BatchRetrieve(index, wmodel=\"Tf\") \n",
        "# BM25F = pt.BatchRetrieve(index, wmodel=\"BM25F\") \n",
        "# XSqrA_M = pt.BatchRetrieve(index, wmodel=\"XSqrA_M\") \n",
        "# DirichletLM = pt.BatchRetrieve(index, wmodel=\"DirichletLM\") \n",
        "# LemurTF_IDF = pt.BatchRetrieve(index, wmodel=\"LemurTF_IDF\") \n",
        "# PL2 = pt.BatchRetrieve(index, wmodel=\"PL2\") \n",
        "# PL2F = pt.BatchRetrieve(index, wmodel=\"PL2F\") \n",
        "# BB2 = pt.BatchRetrieve(index, wmodel=\"BB2\") \n",
        "# Dl = pt.BatchRetrieve(index, wmodel=\"Dl\") \n",
        "# DLH = pt.BatchRetrieve(index, wmodel=\"DLH\") \n",
        "# DLH13 = pt.BatchRetrieve(index, wmodel=\"DLH13\") \n",
        "# DPH = pt.BatchRetrieve(index, wmodel=\"DPH\") \n",
        "# CoordinateMatch = pt.BatchRetrieve(index, wmodel=\"CoordinateMatch\") \n",
        "# DFIC = pt.BatchRetrieve(index, wmodel=\"DFIC\") \n",
        "# DFIZ = pt.BatchRetrieve(index, wmodel=\"DFIZ\") \n",
        "# DFR_BM25 = pt.BatchRetrieve(index, wmodel=\"DFR_BM25\") \n",
        "# DFRee = pt.BatchRetrieve(index, wmodel=\"DFRee\") \n",
        "# DFReeKLIM = pt.BatchRetrieve(index, wmodel=\"DFReeKLIM\") \n",
        "# DFRWeightingModel = pt.BatchRetrieve(index, wmodel=\"DFRWeightingModel\") \n",
        "# In_expB2 = pt.BatchRetrieve(index, wmodel=\"In_expB2\") \n",
        "# In_expC2 = pt.BatchRetrieve(index, wmodel=\"In_expC2\") \n",
        "# InB2 = pt.BatchRetrieve(index, wmodel=\"InB2\") \n",
        "# InL2 = pt.BatchRetrieve(index, wmodel=\"InL2\") \n",
        "# InL2 = pt.BatchRetrieve(index, wmodel=\"InL2\") \n",
        "# LGD = pt.BatchRetrieve(index, wmodel=\"LGD\") \n",
        "# MDL2 = pt.BatchRetrieve(index, wmodel=\"MDL2\") \n",
        "# ML2 = pt.BatchRetrieve(index, wmodel=\"ML2\") \n",
        "# Hiemstra_LM = pt.BatchRetrieve(index, wmodel=\"Hiemstra_LM\") \n",
        "# IFB2 = pt.BatchRetrieve(index, wmodel=\"IFB2\") \n",
        "# null = pt.BatchRetrieve(index, wmodel=\"Null\") "
      ],
      "metadata": {
        "id": "TKqQNp_D1qia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
